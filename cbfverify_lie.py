# -*- coding: utf-8 -*-
"""CBFverify_Lie.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IGmsn386wsM4WwS8o3CEqckkRwUKOv_y

# Verify the conditions of CBF - part 2:
* Goal: $\dot B(x) < 0, \; \forall x \in \{x|B(x) = 0\}$

* Approach: Find $UB$ and $LB$ s.t. $LB \leq \dot B(x) \leq UB$ 
  * note: $\dot B(x) = \sum_i \frac{\partial B(x)}{\partial x_i} \frac{dx_i}{dt}$. In this notebook, we implement functions that can find the intervals of $\frac{\partial B(x)}{\partial x_i}$ only.  
  * $\frac{\partial B(x)}{\partial x_i} = \sum_k W_{1,k}^{(2)} \sigma^\prime(W_{k.:}^{(1)}x+b_k^{(1)}) W_{k.i}^{(1)} = W_{1,:}^{(2)} \left[ \sigma^\prime(W^{(1)}x+b^{(1)}) \odot W_{:,i}^{(1)} \right]$

* Implemented interval bounds:
  * get_lie_derivative_bounds

* Usage: (support batch dimension for $x_0$)
```
UBs, LBs = get_lie_derivative_bounds(x0, eps, Ws, bs) # UBs & LBs shape: (num_batch, dim_in, 1)
```

"""

""" ## Import """
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

from cbfverify_barrier_fastlin import get_weights

"""## Setup

### Lie derivative bound
"""

def dev_relu(x):
  x[x<0] = 0
  x[x>0] = 1
  return x

def get_lie_derivative_bounds(x0, eps, Ws, bs, activation='relu'):
  # initial setting
  if activation == 'relu':
    act = dev_relu
  elif activation == 'tanh':
    act = lambda x: 1-torch.tanh(x)**2 
  else:
    assert False, 'The activation function {} is not supported'.format(activation)
  
  # propagate linear layer
  mid = torch.matmul(Ws[0], x0) + bs[0]
  shift = torch.norm(eps*Ws[0],p=1,dim=1) # dim = 1, compute row norm
  LB_0 = mid - shift.unsqueeze(1)  # shape: (num_hidden, num_batch)
  UB_0 = mid + shift.unsqueeze(1)

  # propagate non-linear activation: tanh'
  LB_1 = torch.min(act(LB_0), act(UB_0)) # shape: (num_hidden, num_batch)
  UB_1 = torch.max(act(LB_0), act(UB_0))
  idx_uns = torch.logical_and(LB_0 < 0, UB_0 > 0)
  UB_1[idx_uns] = 1

  # propagate the weight element-wise multiplication:
  W_pos1=Ws[0].clone()
  W_neg1=Ws[0].clone()
  W_pos1[Ws[0]<0] = 0
  W_neg1[Ws[0]>0] = 0
  UB_2 = torch.bmm(W_pos1.unsqueeze(2),UB_1.unsqueeze(1))+ torch.bmm(W_neg1.unsqueeze(2),LB_1.unsqueeze(1))  
  LB_2 = torch.bmm(W_pos1.unsqueeze(2),LB_1.unsqueeze(1))+ torch.bmm(W_neg1.unsqueeze(2),UB_1.unsqueeze(1)) 
  # shape: (num_hidden, num_out, num_batch)

  # propagate final linear layer:
  W_pos2 = Ws[1].clone()
  W_neg2 = Ws[1].clone()
  W_pos2[Ws[1]<0] = 0
  W_neg2[Ws[1]>0] = 0
  UB_3 = torch.matmul(UB_2.T,W_pos2.T)+torch.matmul(LB_2.T,W_neg2.T)
  LB_3 = torch.matmul(LB_2.T,W_pos2.T)+torch.matmul(UB_2.T,W_neg2.T)

  return UB_3, LB_3

"""### Forward prop $\dot B(x)$"""

def forward_prop_lie(Ws,bs,x0,activation='relu'):
  if activation == 'relu':
    act = dev_relu
  elif activation == 'tanh':
    act = lambda x: 1-torch.tanh(x)**2 
  else:
    assert False, 'The activation function {} is not supported'.format(activation)

  out = torch.matmul(Ws[0],x0) + bs[0]
  out = act(out)
  out = torch.bmm(Ws[0].unsqueeze(2),out.unsqueeze(1))
  out = torch.matmul(out.T,Ws[1].T)
  # print(out.shape)
  return out


"""## Testing
### 1. random samples $x_{samp}$, check $LB_i \leq \dot B_i(x_{samp}) \leq UB_i$ for all $i$ and $x_{samp}$
"""

def generate_random_sample(x0, eps):
  # input: [dim, batch]
  # rand: [0,1) --> make input range: (-1, 1)
  x_samples = eps*(2*torch.rand(x0.shape)-1)+x0  # x_sample: shape (dim_in, num_batch)
  return x_samples


"""### 2. MC sampling: plot out the figure"""

def show(x,y,bnds_x,bnds_y):
  plt.figure()
  plt.scatter(x,y,marker='o')
  plt.scatter(bnds_x,bnds_y,c="red")
  # plt.xlim(0.1,0.3)
  # plt.ylim(0,0.2)
  plt.show()


"""### 3. Wrap as a function"""

def validate_bounds(x0, eps, Ws, bs):
  # get bounds
  UBs, LBs = get_lie_derivative_bounds(x0, eps, Ws, bs)

  # generate MC samples
  x_samples = generate_random_sample(x0, eps)
  out_samples = forward_prop_lie(Ws,bs,x_samples)
   
  bounds = torch.cat((LBs[0],UBs[0]),1) # bounds[i]: (LB_i, UB_i)
  # show(out_samples[:,0,0].numpy(),out_samples[:,1,0].numpy(),bounds[0].numpy(),bounds[1].numpy())

  # check violations
  violation_UBs = torch.where(UBs < out_samples)
  violation_LBs = torch.where(LBs > out_samples)

  # print("UBs[8]:{}, LBs[8]:{}".format(UBs[8],LBs[8]))
  # print("out_samples[:,0,0]:{}".format(out_samples[:,0,0]))

  for i in range(len(violation_UBs)):
    assert torch.sum(violation_UBs[i]) == 0, "violating UBs[{}]".format(i)  
  print("pass validation of UB!")

  for i in range(len(violation_LBs)):
    assert torch.sum(violation_LBs[i]) == 0, "violating LBs[{}]".format(i)
  print("pass validation of LB!") 

  return violation_UBs, violation_LBs


if __name__ == "__main__":
  
  """### Initialization"""  
  ### Load model
  dim_in, dim_out, num_hidden, num_batch = 3, 1, 128, 20
  model = nn.Sequential(nn.Linear(dim_in,num_hidden),nn.Tanh(),nn.Linear(num_hidden,dim_out))
  Ws, bs = get_weights(model)  

  # input: [dim, batch]
  x0 = torch.ones([dim_in, num_batch], dtype=torch.float32)
  # perturbation magnitude
  eps = 0.01
  """### Get Lie derivative intervals"""
  UBs, LBs = get_lie_derivative_bounds(x0, eps, Ws, bs) # UBs & LBs shape: (num_batch, dim_in, 1)
  
  
  """### playground of testing """
  # shape: (num_batch, dim_in, 1) 
  outs = forward_prop_lie(Ws,bs,x0)
  
  x_samples = generate_random_sample(x0, eps)
  out_samples = forward_prop_lie(Ws,bs,x_samples) # shape: (num_batch, dim_in, 1)

  # check if all UBs > f(x0)
  torch.where(UBs < out_samples)

  # check if all LBs < f(x0)
  torch.where(LBs > out_samples)
  bounds = torch.cat((LBs[0],UBs[0]),1) # bounds[i]: (LB_i, UB_i)
  show(out_samples[:,0,0].numpy(),out_samples[:,1,0].numpy(),bounds[0].numpy(),bounds[1].numpy())

  """### start testing"""
  num_test = 0
  if num_test:
    num_sample = 1000
    x0 = torch.rand(dim_in,1)
    x0 = x0.repeat(1,num_sample)
    v0, v1 = validate_bounds(x0,0.01,Ws,bs)
    v0, v1 = validate_bounds(x0,0.2,Ws,bs)
  
  